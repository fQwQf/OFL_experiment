# Explosions in SVHN

我们在 SVHN 上测试了 V12，出现了意料之外的结果。

算法V12的核心思想之一是动态任务衰减。其本意是：在训练早期，给予对齐损失（ align_loss ）较高的权重，强制各个客户端的类原型向全局一致的ETF锚点对齐；随着训练的进行，逐渐降低这个权重，让模型更专注于学习本地数据中的具体特征。

然而，从日志中我们可以清晰地看到，实际发生的情况与这个目标完全相反。

1. Lambda 失控：
  我们来看日志中关键的 Raw lambda 和 Truly Effective lambda 。 Raw lambda 是由模型动态学习出的两个任务（本地任务和对齐任务）的相对重要性，而 Truly Effective lambda 是 Raw lambda 经过退火因子 s(p) 衰减后，最终作用于对齐损失的权重。

  *  理想情况： Raw lambda 应该在一个合理的范围内动态调整，而 Truly Effective lambda 应该随着 s(p) 的减小而稳步下降。
  *  实际情况：以 Client 3 为例， Raw lambda 的值从最初的 12.04 (Round 0) 开始，一路失控飙升，到第14轮已经达到 204658.41 ，在第19轮更是达到了惊人的 1516252.51 。

  | Round | Client 3 Raw lambda | s(p) | Client 3 Truly Effective lambda | Test Accuracy |
  | :--- | :--- | :--- | :--- | :--- |
  | 0 | 12.04 | 0.980 | 11.80 | 0.259 |
  | 9 | 65.22 | 0.800 | 52.17 | 0.454 |
  | 14 | 204,658.41 | 0.700 | 143,260.89 | 0.495 |
  | 15 | 482,429.48 | 0.680 | 328,052.05 | 0.375 |
  | 19 | 1,516,252.51 | 0.600 | 909,751.50 | 0.263 |
  | 29 | 4,147,315.36 | 0.400 | 1,658,926.14 | 0.188 |

2. 性能与 lambda 的相关性：
  从上表可以看出一个清晰的趋势：
  *  在训练初期（约0-14轮）， Truly Effective lambda 还在一个“巨大但尚未完全失控”的范围内，测试准确率一路上升，在第14轮达到约 49.5% 的峰值。
  *  从第15轮开始， Truly Effective lambda 的增长进入了失控状态，其数值变得过大。与此同时，模型的测试准确率应声下跌，一路从近50%跌至最后几轮的16%左右，说明模型学到的知识被严重破坏了。

### 为什么 lambda 会爆炸

lambda 的计算公式为 effective_lambda = (sigma_sq_local / sigma_sq_align) 。它的爆炸意味着分子 sigma_sq_local 变得极大，或者分母 sigma_sq_align 变得极小。这通常发生在两个损失项的数值或梯度差异巨大时。

在 SVHN 且 alpha=0.05 的设定下，可能发生了以下情况：

1. 庞大且复杂的本地损失 ( base_loss )： base_loss 由分类损失、对比损失等四个部分组成。在数据高度倾斜的客户端上，这个损失可能非常大且难以优化。根据不确定性加权的原理，为了降低总损失，优化器会倾向于调高 sigma_sq_local 的值。
2. 简单且稳定的对齐损失 ( align_loss )： align_loss 是一个简单的均方误差损失，目标是将客户端的类原型拉向一个固定的ETF锚点。这个任务相对简单， align_loss 的值可能很小，或者下降得很快。因此，优化器会倾向于急剧调低 sigma_sq_align 的值。

当 sigma_sq_local 持续增大，而 sigma_sq_align 持续减小时，它们的比值 lambda 就会出现指数级的爆炸。

当 Effective lambda 变得极其巨大时，总损失函数 loss = base_loss + Truly Effective lambda * align_loss 会被 align_loss 这一项完全主导。

这意味着：模型放弃了学习数据特征，优化器唯一的目地变成了不惜一切代价降低 align_loss 。它会强制模型生成的类原型（ learnable_proto ）与固定的ETF锚点（ fixed_anchors ）在数值上完全一致。同时，特征提取器失去意义，为了满足这个苛刻的对齐要求，模型不再学习如何从SVHN的数字图像中提取有用的、可区分的特征。它学习到的所有能力都被用来满足这个几何约束。一个只会将所有样本映射到几个固定向量（ETF锚点）的模型，是没有任何泛化能力的。这解释了为什么在lambda爆炸后，测试集准确率会断崖式下跌。

V12 在SVHN上的失败，其核心问题在于：

在SVHN高度非同质的数据上，本地任务和全局对齐任务的难度差异过大，导致了不确定性加权机制的崩溃，使对齐任务的权重增长到不合理的程度，最终迫使模型放弃了从数据中学习，导致性能崩溃。

为了解决问题，我们在总损失中加入了一个正则项 gamma_reg / sigma_sq_align。 lambda 爆炸的原因之一是分母 sigma_sq_align 变得过小（趋近于0），代表模型对“对齐”这个任务变得“过度自信”。这个正则项直接惩罚过小的 sigma_sq_align 。当 sigma_sq_align -> 0 时， 1 / sigma_sq_align -> ∞，产生巨大的惩罚，从而阻止 sigma_sq_align 的崩溃。
