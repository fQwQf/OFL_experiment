在测试中我们发现性能仍然存在改进空间，于是我们又做了一些改进。

我们之前的修正（V13.1）只是给分母 σ²_align 加了一个“地板”，但没有给分子 σ²_local 加一个“天花板”，更没有对最终的比值 lambda 进行整体约束。因此，尽管训练的爆炸推迟了，但是依然发生。于是我们又进行了以下两个方向的调整。

---

### 为分子 σ²_local 加入正则项

*  核心思想: lambda 爆炸的原因之一是 σ²_local （分子）可以无代价地增长。如果我们能像约束 σ²_align 一样约束 σ²_local ，就能从根本上限制 lambda 的上限。

 σ²_local 的问题是可能无限增大，尤其是在 base_loss 很小时。因此，它需要一个“天花板”式的正则项。直接惩罚 sigma_sq_local 本身（或其对数形式）的增长才是正确的做法。对 log(σ²) 的L2正则化正是实现这一目标的标准且有效的方法。

 stability_anchor_align 负责分母的稳定性， ceiling_anchor_local 负责分子的稳定性。当 base_loss 趋近于0时， ceiling_anchor_local 将成为抑制 σ²_local 增长的主要力量，阻止其走向无穷大。同时， stability_anchor_align 确保 σ²_align 不会崩溃。 lambda = σ²_local / σ²_align 因此被有效地限制在一个合理的范围内。


### 将 lambda 整体加入正则项进行约束

我们为什么不直接惩罚我们不想要的行为——即过大的 lambda 值本身呢？

核心思想是，在损失函数中明确加入一项，当 lambda 超过某个我们认为合理的范围时，就对其进行惩罚。首先计算 Effective lambda ，以便梯度能够回传给 σ 参数。然后设计一个惩罚函数。一个简单有效的函数是 ReLU ，即只惩罚超过阈值的部分。

这样极其直接和可解释。我们的目标就是控制 lambda ，这个正则项就是直接控制 lambda 。它的行为非常容易理解和调试。并且 ReLU 惩罚提供了一个软上限。一旦 lambda 试图超过 lambda_max_threshold ，就会有一个强大的梯度把它拉回来。它也可以与现有的 stability_anchor 共存。前者防止分母为0造成 nan ，后者防止爆炸（处理上限），共同构建了一个极为鲁棒的系统。


于是我们进行了实验。

### 实验结果分析

我们将三组实验并列进行分析：
*  V12 : 原始的算法。
*  V12.1 : 引入了分子 σ²_local 正则项的修正算法。
*  V13 : 施加了强正则化的修正算法。

#### Raw lambda 的行为

| 实验版本 |  Raw lambda at Round 14 | Raw lambda at Round 20 | 
| :--- | :--- | :--- | 
| V12 | 204,658 | 1,771,644 | 
| V12.1 | 123,086 | 530,660 | 
| V13 | 50.40 | 50.03 | 



#### 测试准确率

| 实验版本 | gamma_reg | 峰值准确率 (所在轮次) | 最终准确率 (第49轮) |
| :--- | :--- | :--- | :--- |
| V12 (原始) | 0.0 | 49.5% (Round 14) | 16.4% | 
| V13 (中等) | 1e-5 | 50.0% (Round 13) | 17.7% | 
| V14 (强) | 0.001 | 55.4% (Round 26) | 52.9% |


### 分析结论

1. 我们引入的 lambda ReLU 完全解决了 lambda 的爆炸性增长问题。 Raw lambda 从动辄数十万的量级，被成功地“锚定”在了 50 左右这样一个极其合理的数值范围内。
2. 分子 σ²_local 加入正则项的实验表明，虽然正则化的方向是正确的，但正则化的强度必须足够大才能对抗优化器将 sigma_sq_align 推向零的强大趋势。与之相比，直接对 lambda 加入 ReLU 更为有效。

---

### 最终结论

1. 在极端Non-IID环境下，当一个任务（ base_loss ）的损失信号过早消失时，优化器会无限地、单方面地增大另一个任务（ align_loss ）的权重，造成lambda失衡。

2. 简单地为 lambda 的组成部分（分子或分母）提供数值稳定性是不够的。对 lambda 这个最终的、动态的权重本身进行直接的、有上限的正则化约束，则能在不破坏自适应学习灵活性的前提下，保证整个系统的全局稳定性。

3. V14是鲁棒的、可泛化的SOTA算法:
  *  它解决了V12在SVHN数据集上暴露的数值爆炸问题。
  *  它在SVHN上取得了稳定且高性能的结果（峰值~54.6%），远超之前的SOTA V7（约51%）。
  *  它的 lambda ReLU 机制在CIFAR-100等正常数据集上不会被激活（因为 lambda 本身就不会达到阈值），证明了其非侵入性和广泛的适用性。

V14 标志着我们的自适应框架从一个理论模型演变成了一个健壮的、可以在多样化和恶劣环境下稳定工作的工程实践。
