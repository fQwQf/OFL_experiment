# DHCL + Lambda Annealing

显然“全局一致”和“本地适应”是一对需要平衡的矛盾，一个固定的 lambda_align 可能在整个训练过程中并非最优。一个更智能的策略是：  

- 在训练早期，客户端模型相差甚远，需要强力的对齐来迅速收敛到一个共同的特征空间（lambda 较大）。
- 在训练后期，模型已经基本对齐，需要给予更多自由度来精调和适应本地数据的细微特征（lambda 较小）。

原始实验数据见[DRCL-with-lambda-annealing.txt](DRCL-with-lambda-annealing.txt)。

### 数据分析

### 1. 最终性能对比 (第50轮)

下表总结了在第50个通信轮次时，三个算法在不同数据异构程度（由 `alpha` 控制）下的核心指标。

| Alpha | 算法 | 最终准确率 | 准确率提升 (Δ vs V4) | 模型方差 | 最终原型标准差 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0.05 | OursV4 | 66.97% | \- | 0.003233 | 1.00672 |
| | OursV5 | 67.44% | +0.47% | 0.003239 | 0.93532 |
| | OursV6 | 68.17% | +1.20% | 0.003296 | 0.70938 |
| 0.1 | OursV4 | 76.10% | \- | 0.003697 | 1.00676 |
| | OursV5 | 76.13% | +0.03% | 0.003703 | 0.92177 |
| | OursV6 | 75.95% | -0.15% | 0.003741 | 0.65501 |
| 0.3 | OursV4 | 83.90% | \- | 0.004826 | 1.00671 |
| | OursV5 | 84.07% | +0.17% | 0.004818 | 0.89341 |
| | OursV6 | 84.54% | +0.64% | 0.004884 | 0.55652 |
| 0.5 | OursV4 | 87.69% | \- | 0.005401 | 1.00673 |
| | OursV5 | 87.72% | +0.03% | 0.005393 | 0.87582 |
| | OursV6 | 88.17% | +0.48% | 0.005437 | 0.50950 |

### 2. 学习动态：准确率演进过程

| Alpha | 算法 | 第10轮 Acc | 第30轮 Acc | 第50轮 Acc |
| :--- | :--- | :--- | :--- | :--- |
| 0.05 (极高) | OursV4 | 49.73% | 62.37% | 66.97% |
| | OursV5 | 49.86% | 61.36% | 67.44% |
| | OursV6 | 50.93% | 63.65% | 68.17% |
| 0.1 (高) | OursV4 | 54.65% | 70.24% | 76.10% |
| | OursV5 | 53.31% | 71.02% | 76.13% |
| | OursV6 | 54.47% | 70.15% | 75.95% |
| 0.3 (中等) | OursV4 | 65.87% | 79.67% | 83.90% |
| | OursV5 | 65.92% | 78.89% | 84.07% |
| | OursV6 | 66.63% | 79.33% | 84.54% |
| 0.5 (低) | OursV4 | 73.66% | 84.74% | 87.69% |
| | OursV5 | 74.36% | 84.67% | 87.72% |
| | OursV6 | 74.34% | 85.22% | 88.17% |

### 3. 实验分析

#### a. 准确率分析： lambda 退火策略全面胜出

*   在高异构性 (α=0.05, 0.3) 下，`OursV6` 优势巨大：在 `α=0.05` 时，`OursV6` 相比原始FAFI (`V4`) 实现了 +1.20% 的显著提升，并且也优于 `V5`。这证明了我们的核心假设：训练早期强对齐、后期弱对齐的策略，完美契合了高异构场景的需求。它首先强行将模型拉到同一个“起跑线”，然后在接近终点时允许它们进行个性化的“冲刺”。
*   在低异构性 (α=0.5) 下，`OursV6` 依然是最佳选择：即使在数据分布较为均衡的情况下，`OursV6` 仍然取得了最高的准确率 (88.17%)。这说明退火策略不仅解决了高异构的问题，其“后期微调”的特性也很好地保留了模型适应本地数据的能力，找到了比固定 lambda 的`V5`更优的平衡点。
*   唯一的例外 (α=0.1)：在此设定下，`OursV6` 的最终表现略低于V4和V5。这可能是由于超参数（初始 lambda 、退火速率）与该特定数据分布未能完美匹配。但考虑到其差距极小 (-0.15%) 且在其他所有场景中均获胜，这更像是一个需要微调的特殊案例，而非策略本身的失败。
*   学习动态：从学习过程来看，`OursV6` 在几乎所有场景的早期阶段（第10轮）就取得了领先，这表明强初始对齐策略能加速模型进入一个更优的收敛轨道。

#### b. 模型一致性分析：达到了前所未有的对齐水平

这是最令人振奋的结果，它从根本上验证了我们方法的有效性。

*   全局原型标准差 (`g_protos_std`) 大幅降低：
    *   在所有alpha设置下，`OursV6` 的 `g_protos_std` 都达到了最低值，远低于V5，更是将V4远远甩开。例如在 `α=0.5` 时，`g_protos_std` 从V4的 `1.006` 降至 `0.509`，几乎降低了50%。
    *   这证明了 lambda 退火策略是实现客户端模型（原型）对齐的极其有效的手段。

*   模型参数方差 (Model Variance) 的微妙变化：
    *   `OursV6` 的模型方差略高于V4和V5。我们推测，目前的策略在保证分类决策核心（原型）高度一致的前提下，允许特征提取器（模型的其余参数）在后期有更大的自由度去适应本地数据，使得最终的参数呈现出有益的“个性化”。因此，我们可以得出结论：OursV6 成功地做到了 “特征空间结构一致，但模型具体参数存有个性” 的理想状态。