# DHCL + ETF

V. Papyan, X.Y. Han, & D.L. Donoho, Prevalence of neural collapse during the terminal phase of deep learning training, Proc. Natl. Acad. Sci. U.S.A. 117 (40) 24652-24663, https://doi.org/10.1073/pnas.2015509117 (2020).  
这是“神经塌陷”的开创性论文，它揭示了ETF结构是深度学习分类任务的最终归宿。

Z. Li, X. Shang, R. He, T. Lin and C. Wu, "No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier," 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Paris, France, 2023, pp. 5296-5306, doi: 10.1109/ICCV51070.2023.00490.  
这篇论文提出，使用一个固定的、权重形成ETF结构的分类器可以极大地缓解Non-IID问题。

神经塌陷理论指出：
1. 特征塌陷 (Feature Collapse): 同一个类别的所有样本的特征，最终会塌陷到它们的均值点（即原型）。
2. 原型最优分离 (Prototype Optimality): 所有类别的原型在特征空间中会相互尽可能地远离，形成一种被称为“单纯形等角紧框架” (Simplex Equiangular Tight Frame, ETF) 的完美几何结构。
3. 分类器对齐 (Classifier Alignment): 最终分类器（在我们的案例中是learnable_proto）的权重向量会与对应的类别原型对齐。

很容易想到：既然一个完美训练好的模型的分类器权重（原型）最终会自发形成ETF结构，可以直接创建一个ETF作为固定的锚点，让所有客户端从一开始就向其对其。  

### 实验数据

原始数据见[DRCL-ETF.txt](DRCL-ETF.txt)。

### 1. 最终性能对比 (第50轮)

下表汇总了所有算法变体在第50轮的核心指标。

| 异构程度 (α) | 算法 | 最终准确率 (Acc) | 准确率提升 (Δ vs V4) | 模型方差 (Mean) | 全局原型标准差 (Std) |
| :--- | :--- | :--- |:--- | :--- | :--- |
| 0.05 (极高) | OursV4 | 66.97% | \- | 0.003233 | 1.00672 |
| | OursV5 | 67.44% | +0.47% | 0.003239 | 0.93532 |
| | OursV6 | 68.17% | +1.20% | 0.003296 | 0.70938 |
| | OursV7 | 67.77% | +0.80% | 0.003293 | 0.70977 |
| 0.1 (高) | OursV4 | 76.10% | \- | 0.003697 | 1.00676 |
| | OursV5 | 76.13% | +0.03% | 0.003703 | 0.92177 |
| | OursV6 | 75.95% | -0.15% | 0.003741 | 0.65501 |
| | OursV7 | 76.86% | +0.76% | 0.003730 | 0.65451 |
| 0.3 (中等) | OursV4 | 83.90% | \- | 0.004826 | 1.00671 |
| | OursV5 | 84.07% | +0.17% | 0.004818 | 0.89341 |
| | OursV6 | 84.54% | +0.64% | 0.004884 | 0.55652 |
| | OursV7 | 83.57% | -0.33% | 0.004867 | 0.55661 |
| 0.5 (低) | OursV4 | 87.69% | \- | 0.005401 | 1.00673 |
| | OursV5 | 87.72% | +0.03% | 0.005393 | 0.87582 |
| | OursV6 | 88.17% | +0.48% | 0.005437 | 0.50950 |
| | OursV7 | 88.46% | +0.77% | 0.005403 | 0.50973 |

---

### 2. 学习动态：准确率演进过程

| 异构程度 (α) | 算法 | 第10轮 Acc | 第30轮 Acc | 第50轮 Acc |
| :--- | :--- | :--- | :--- | :--- |
| 0.05 (极高) | OursV6 | 50.93% | 63.65% | 68.17% |
| | OursV7 | 48.50% | 62.12% | 67.77% |
| 0.1 (高) | OursV6 | 54.47% | 70.15% | 75.95% |
| | OursV7 | 56.20% | 71.61% | 76.86% |
| 0.3 (中等) | OursV6 | 66.63% | 79.33% | 84.54% |
| | OursV7 | 66.38% | 79.56% | 83.57% |
| 0.5 (低) | OursV6 | 74.34% | 85.22% | 88.17% |
| | OursV7 | 73.53% | 85.42% | 88.46% |

### 结论

`OursV7` (ETF锚点) 成功地解决了`OursV6` (随机锚点) 在`α=0.1`时的不稳定性问题，证明了“最优几何结构”锚点的鲁棒性。 然而，在最极端的异构场景 (`α=0.05`) 下，`OursV6`的随机锚点反而表现更优。这可能是由于在某些极端条件下一个次优但“更容易达到”的目标可能效果更好。


### 1. 准确率分析

*   α=0.1, 0.5:
    *   在`α=0.1`时，`OursV7`取得了 +0.76% 的提升，修复了`OursV6`在该点的性能凹陷。在`α=0.5`时，它也以 +0.77% 的优势超越了基线。
    *   这证明了ETF锚点的鲁棒性。一个几何上完美的、对称的、类别间最大化分离的目标，为大多数Non-IID场景提供了一个稳定、高质量的对齐目标。与`OursV6`的随机锚点相比，`OursV7`的表现更可预测、更可靠。

*   α=0.05, 0.3:
    *   在最极端的`α=0.05`下，`OursV6`反超`OursV7`约0.4%。在`α=0.3`时也出现了类似情况。
    *   假设为：“神经塌陷”理论描述的是在理想条件下（如IID数据、无限训练）的最终状态。然而，在`α=0.05`的极端Non-IID条件下，每个客户端的数据极其匮乏。强迫对齐一个几何上完美但可能距离其初始状态非常遥远的ETF目标，可能是一个过强的、不切实际的约束。
    *   相比之下，`OursV6`的随机锚点，虽然理论上不完美，但它在特征空间中的位置是任意的。它有一定概率会恰好落在离所有客户端初始状态都不太远的一个位置，从而为这些能力受限的本地模型提供了一个更容易达成的对齐目标。

### 2. 模型一致性分析

*   `g_protos_std` 几乎完全一致：`OursV6`和`OursV7`的全局原型标准差低得惊人，且彼此之间几乎没有差异。
*   这说明 lambda 退火机制是实现模型一致性的主要原因。无论目标是随机的还是ETF，这个机制都能强有力地将客户端原型拉到一起。因此，性能的差异几乎完全取决于对齐目标本身的质量和可达性。