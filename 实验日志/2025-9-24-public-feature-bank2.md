# 2025/9/24 public feature bank 2

### 执行摘要

实验结果明确显示，之前提出的两阶段训练方案（`OursV6`）**在所有Non-IID设置下，性能均显著低于**原始的FAFI基线（`OursV4`）。更重要的是，数据的Non-IID程度越强（`alpha`值越小），`OursV6`的性能下降越剧烈。

---

## **实验设置概述**

*   **数据集：** CIFAR-10
*   **模型：** ResNet18
*   **客户端数量：** 5
*   **本地训练轮次 (local_epochs)：** 5（首次为对齐阶段）
*   **联邦学习轮次 (num_rounds)：** 5
*   **Non-IID程度 (`alpha`)：** 0.5 (较弱非IID), 0.3 (中等非IID), 0.05 (强非IID)
*   **对比学习温度 (`contrastive_temperature`)：** 0.5
*   **公共数据比：** 0.1
*   **特征库大小：** 4096
*   **服务器教师模型的预训练轮数：** 20

### 实验结果对比

| Non-IID Level (alpha) | Round | **OursV4** (Baseline) | **OursV6** (Two-Stage) | Performance Delta (V6 - V4) |
| :--- | :---: | :---: | :---: | :---: |
| **0.5 (弱)** | 0 | **67.23%** | 57.68% | -9.55% |
| | 1 | **76.70%** | 71.27% | -5.43% |
| | 2 | **81.13%** | 76.92% | -4.21% |
| | 5 | **86.79%** | 84.45% | -2.34% |
| **0.3 (中)** | 0 | **65.16%** | 47.65% | -17.51% |
| | 1 | **74.79%** | 60.79% | -14.00% |
| | 2 | **79.09%** | 66.91% | -12.18% |
| | 5 | **85.62%** | 75.96% | -9.66% |
| **0.1 (强)** | 0 | **61.00%** | 45.38% | -15.62% |
| | 1 | **71.24%** | 53.19% | -18.05% |
| | 2 | **76.98%** | 59.88% | -17.10% |
| | 5 | **83.52%** | 70.01% | -13.51% |
| **0.05 (极强)** | 0 | **61.37%** | 31.25% | **-30.12%** |
| | 1 | **72.25%** | 38.67% | **-33.58%** |
| | 2 | **77.12%** | 42.28% | **-34.84%** |
| | 5 | **83.46%** | 50.13% | **-33.33%** |

---

### 深入分析：假设的彻底失败与洞察

**我们的核心假设是**：通过设置一个专门的“对齐阶段”（Stage 1），可以让客户端模型先学习一个全局一致的特征表示，然后再在“微调阶段”（Stage 2）学习本地任务，从而避免梯度冲突。

**实验结果无情地粉碎了这个假设。** 原因在于一个我们忽略了的关键问题：**预热不足与灾难性遗忘 (Insufficient Warm-up & Catastrophic Forgetting)**。

#### 1. 致命缺陷：对齐阶段太短，无法形成有效的全局“烙印”

在我们的设置中 (`local_epochs: 5`, `alignment_epochs: 1`)，客户端模型在随机初始化后，只用**一个**本地轮次（epoch）的时间来进行全局对齐。

*   **日志分析（`alpha=0.05`, Client 0, Epoch 0）**:
    *   `Epoch 0: Running in Stage 1 - Global Alignment Only.`
    *   `train accuracy: 0.0; test accuracy: 0.1022`

    经过一个轮次的纯对齐训练后，模型在训练集和测试集上的表现**等同于随机猜测**。这意味着，这个所谓的“对齐”阶段，**完全没有让模型学到任何有意义的特征**。它只是将一个随机初始化的网络，变成了另一个同样随机、只是在某个高维空间中与“公共噪声”保持了一点距离的网络。

#### 2. “灾难性遗忘”的反向应用：本地任务迅速覆盖全局信号

当训练进入第二阶段（本地微调）时，一个几乎是随机的模型，突然被暴露在**梯度极其巨大且方向单一**的本地分类损失（`cls_loss`）之下。

*   对于`alpha=0.05`的客户端，其本地数据可能只有{猫，狗}。`cls_loss`的梯度会以压倒性的力量，强迫整个网络的所有参数只为“区分猫和狗”这一个目标服务。
*   在第一阶段学到的、本就微弱不堪的“全局对齐”知识，会在第二阶段的第一个批次（batch）中被彻底冲刷和覆盖，这就是一种**反向的灾难性遗忘**——强大的本地任务信号遗忘了微弱的全局预训练信号。

#### 3. 为什么`OursV6`比`OursV4`更差？——“浪费的关键起步期”

现在我们可以解释为什么`OursV6`甚至比基线更糟糕：

*   **`OursV4`**: 从第一个epoch开始，就同时接收`cls_loss`等所有损失的监督。尽管存在梯度冲突，但至少模型从一开始就在学习如何**为本地数据进行分类**，这是一个有意义的任务。它有**5个完整的epoch**来学习这个任务。
*   **`OursV6`**: 将宝贵的**第一个epoch完全浪费**在了一个无效的对齐任务上，这个任务结束后模型基本还是随机的。然后，它只剩下**4个epoch**来从零开始学习本地任务。

**`OursV6`不仅没有解决梯度冲突，反而因为浪费了关键的起步学习阶段，导致其学习进度全面落后于`OursV4`。**

这个效应在`alpha=0.05`时最为惨烈，因为此时本地任务的过拟合倾向最强，模型最需要从一开始就建立正确的分类概念。而在`alpha=0.5`时，本地任务与全局任务较为接近，浪费一个epoch的负面影响相对较小，因此性能差距也最小。

---

### 下一步方向：从“分离”到“融合”与“平衡”

这次的失败非常有价值，它告诉我们：在One-shot场景下，**简单的“时间分离”策略是行不通的**，因为没有足够的时间让模型先充分对齐再微调。

因此，我们必须回到“**同时优化**”的思路上来，但需要更智能地**平衡**本地与全局的梯度。

**核心思路**：我们不应该让全局对齐和本地分类成为两个“你死我活”的目标，而是让**全局对齐作为本地分类的正则化项**，引导本地学习过程，而不是主导它。

#### **方案1：课程学习式的损失权重调整 (Curriculum Loss Weighting)**

这个方案最为直接，也最容易实现。它承认两个损失都有用，但它们在不同阶段的重要性不同。

*   **想法**：在本地训练的早期，我们更希望模型探索本地数据的特征，因此本地损失（`cls_loss`, `proto_loss`）的权重应该更高。随着模型对本地数据有了基本认识，我们再逐渐增加全局对齐损失（`contrastive_loss`）的权重，引导它不要“跑偏”。
*   **实施**:
    1.  在`.yaml`中增加权重参数，如`contrastive_loss_weight_initial: 0.1`, `contrastive_loss_weight_final: 1.0`。
    2.  在`ours_local_training.py`的训练循环中，动态计算当前epoch的对比损失权重：
        ```python
        # 在 for e in range(...) 循环内
        current_progress = (e - start_epoch) / local_epochs
        contrastive_weight = initial_weight + (final_weight - initial_weight) * current_progress
        
        # 阶段二的损失计算
        loss = cls_loss + proto_loss + pro_con_loss + (contrastive_weight * contrastive_loss)
        ```
    *   **优点**：实现简单，逻辑清晰，直接解决了梯度主导权的问题。

#### **方案2：自适应权重调整 (Adaptive Weighting)**

这是一个更高级的方案，它根据模型对任务的“信心”来自动调整权重。

*   **想法**：如果模型在本地任务上表现不佳（例如，`cls_loss`很高），说明它还没学好本地知识，此时应该降低全局对齐的权重，让它专注于本地。当本地任务`cls_loss`下降后，再增加全局对齐的权重。
*   **实施**：可以使用一些多任务学习中的自适应权重算法，例如基于梯度的归一化或不确定性加权。这相对复杂，可以作为方案1成功后的下一步探索。