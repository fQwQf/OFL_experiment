# Why v10 fails again

我们此时在TinyImagenet上进行了另一个实验：

### 实验结果 (第9轮):

V4: 0.88%  
V7 (lambda=20): 0.85%  
V9 (lambda~15): 0.91%  
V10 : 1.21%  

| 客户端 | V9 初始 Lambda (预热值) | V10 最终学习到的 Effective Lambda (Round 9) | 
| :--- | :--- | :--- |
| Client 0 | 15.34 | 12.10 |
| Client 1 | 15.71 | 11.68 |
| Client 2 | 15.23 | 11.49 |
| Client 3 | 16.50 | 11.59 |
| Client 4 | 15.87 | 11.78 |

我们自然会从lambda的角度考虑。V7强制使用了 lambda=20.0，V9 使用了 ~15.5 的根据数据生成的固定值，比 V7 好点 (0.91%)，但依然偏大。V10从 ~15.5 开始，自己摸索到了 ~11.5 这个更优的平衡点，因此取得了最好的结果 (1.21%)。

然而这种结果并不能令人满意。仅仅是一个已经经实验验证对结果影响较为微妙的超参的改变怎么会对结果产生这么大的，且方向不确定的影响？  

况且，在之前的DRCL报告中，我们已经经实验证明了：即使是保持一个固定的lambda不退火，性能不会出现严重下降，反而会有所提升。v10除了学习lambda肯定还干了别的什么事情。

我们来审视一下v10的loss。


$$ L_v10 = (0.5 / σ\_local²) * base\_loss + (0.5 / σ\_align²) * align\_loss + log(σ\_local) + log(σ\_align) $$

任何人都能一眼看出问题。base_loss和align_loss前多出来了系数 (0.5 / σ_local²) ，相当于 lr 被改变了。这显然是个错误。

此时让我们重新审视 Tiny-ImageNet 的实验，并用我们最新的理解来分析它。那时一个非常异常的现象：为什么v10反而表现最好，也就有了答案。

我们已经知道原始 V10 有两个主要行为：
1.  动态学习率缩放： 基于 σ_local 的大小来缩放总梯度。
2.  学习静态 lambda： 试图找到一个最优的、相对固定的 Effective Lambda。

Tiny-ImageNet 分类任务本身就比 CIFAR-100 难得多，加上 alpha=0.1 的极端异构性，导致模型在本地训练初期的损失 (base_loss) 极高且极不稳定。使用一个相对较高的学习率 lr=0.05 来处理这种巨大的梯度，极有可能导致梯度爆炸或训练过程的严重不稳定。模型参数被推向了不好的区域，难以收敛。在这种环境下，V10 的动态学习率缩放机制 (0.5 / σ_local²) 面对巨大的 base_loss学习到了一个巨大的 σ_local。这个巨大的 σ_local 使得梯度缩放因子 (0.5 / σ_local²) 变得非常小。结果，V10 自动地、动态地大幅降低了实际生效的学习率，避免了被巨大、嘈杂的梯度所摧毁，从而能够在如此困难的任务上取得一点点进步。

换而言之，V10 在 Tiny-ImageNet 上的成功，并非因为它对复杂数据更鲁棒，而是因为它的缺陷恰好充当了一个隐式的的学习率稳定器，纯属巧合。  

一个简单而直接的修正方法是将整个 V10 损失乘以 2 * σ_local²。我们接下来进行修正后的实验。