# Why v10 doesn't fail this time

我们来分析修正后的实验数据。

### 实验结果总结与对比

实验设置:
*   数据集: CIFAR-100
*   异构性: alpha = 0.05
*   本地训练: local_epochs = 10
*   V10-Rescaled: 使用V9预热，专属sigma_lr，并且损失函数被重新缩放以消除动态学习率效应。

#### 最终性能对比表格 (Round 9)

| 算法版本 | Lambda (λ) 策略 | 最终准确率 |
| :--- | :--- | :--- |
| V4 (无对齐) | N/A | 38.41% |
| V7 (固定λ + 退火) | 固定 λ=20，退火至0 | 40.41% |
| V9 (自适应λ + 退火) | 启发式规则 (λ~14)，退火至0 | 39.83% |
| V10-Rescaled (学习λ) | 预热后在线学习 (无退火) | 38.25% |

V10-Rescaled 的性能 (~38.25%) 大幅超越了原始 V10 (~21.88%)，证明了我们关于V10失败主因的判断是正确的。

然而，修正后的 V10-Rescaled 依然未能超越 V7 和 V9。让我们列出 V10-Rescaled 的 Effective Lambda 演化表格：

| Round | Client 0 (λ) | Client 1 (λ) | Client 2 (λ) | Client 3 (λ) | Client 4 (λ) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0 (预热) | 13.51 | 14.01 | 14.36 | 14.17 | 14.90 |
| 0 (训练后) | 13.18 | 13.53 | 13.57 | 13.36 | 13.97 |
| 1 | 12.63 | 13.00 | 12.73 | 12.49 | 13.23 |
| 2 | 12.12 | 12.53 | 12.07 | 11.80 | 12.67 |
| 3 | 11.66 | 12.13 | 11.46 | 11.28 | 12.15 |
| 4 | 11.28 | 11.78 | 10.99 | 10.83 | 11.77 |
| 5 | 10.93 | 11.49 | 10.69 | 10.43 | 11.41 |
| 6 | 10.61 | 11.00 | 10.38 | 10.07 | 11.08 |
| 7 | 10.32 | 10.54 | 10.10 | 9.75 | 10.83 |
| 8 | 10.06 | 10.75 | 9.82 | 9.51 | 10.53 |
| 9 | 9.81 | 10.54 | 9.60 | 9.28 | 10.29 |
| 总降幅 | -27.4% | -24.8% | -33.1% | -34.5% | -30.9% |

从表格中我们可以清晰地看到：
Effective Lambda 确实在持续、稳定地下降，但下降得非常缓慢。经过了整整10轮（100个本地epoch），它也仅仅从 ~14 的高位下降到了 ~10 左右，而动态退火机制会使v7/v9降至0。

我们自然想到：这是因为 sigma_lr 太低，还是 V10 原理本身的限制？

于是我们在调整了 sigma_lr 后继续实验。

### 实验结果总结与对比

实验设置:
*   数据集: CIFAR-100
*   异构性: alpha = 0.05 (极度 Non-IID)
*   本地训练: local_epochs = 10
*   V10 sigma_lr: 0.005 (从 0.0001 提高了50倍)

#### 最终性能对比表格 (Round 9)

| 算法版本 | Lambda (λ) 策略 | 最终准确率 |
| :--- | :--- | :--- |
| V4 (无对齐) | N/A | 38.41% |
| V7 (固定λ + 退火) | 固定 λ=20，退火至0 | 40.41% |
| V9 (自适应λ + 退火) | 启发式 (λ~14)，退火至0 | 39.83% |
| V10-Rescaled (高sigma_lr) | 预热后快速在线学习 (无退火) | 39.41% |


这次实验的结果 (39.41%)，相比之前有较明显的提升，与 V7 和 V9 已经非常接近，处于同一个性能水平，并且远超最差的 V4。这证明了提高 sigma_lr 确实让 V10-Rescaled 能够更有效地学习。

然而，它依然未能超越带有退火机制的 V7。我们来分析 V10-Rescaled 在高 sigma_lr 下的 Effective Lambda 演化。

| Round | Client 0 (λ) | Client 2 (λ) | Client 4 (λ) |
| :--- | :--- | :--- | :--- |
| 0 (预热) | 13.51 | 14.36 | 14.90 |
| 0 (训练后) | 11.87 | 11.80 | 12.43 |
| 1 | 10.93 | 11.08 | 12.07 |
| 2 | 10.40 | 10.32 | 11.76 |
| 3 | 9.89 | 9.57 | 10.68 |
| 4 | 9.68 | 9.17 | 10.38 |
| 5 | 9.06 | 8.43 | 9.57 |
| 6 | 8.68 | 8.35 | 9.41 |
| 7 | 8.42 | 8.37 | 9.41 |
| 8 | 8.44 | 8.74 | 9.89 |
| 9 | 8.42 | 8.43 | 9.41 |
| 总降幅 | -37.7% | -41.3% | -36.9% |

与上次低 sigma_lr 的实验相比，lambda 的下降速度明显加快，最终收敛到了一个更低的值（~9.0 左右，上次是 ~10.0 左右）。这证明了 sigma_lr 确实控制着学习速度。但是lambda依然没有归零。尽管学习速度加快，但 V10 仍然没有表现出向零衰减的趋势。lambda 值在后期（Round 8 -> Round 9）基本趋于稳定，甚至 Client 4 的值还略有回升。

我们可以得出结论：V10 的原理本身就是寻找一个最优的“平衡点”，而不是执行一个“动态调度”。

## 总结

我们可以得出以下结论：

1.  对齐的必要性： 在 Non-IID 环境下，强制特征空间对齐是必须的。所有带对齐的版本（V5+）都优于无对齐的 V4。

2.  V10 的内在缺陷：
    *   原始 V10： 其理论包含了有害的梯度缩放效应，这等同于动态地将学习率压得过低，导致模型严重不收敛。
    *   修正后的 V10 (V10-Rescaled)： 去除梯度缩放后，V10 变成了一个纯粹的 lambda 学习器。然而，它的设计目标是收敛到一个静态的最优 lambda。

3.  退火策略的胜利：
    *   V7 和 V9 之所以能取得最佳性能，其最关键的因素是它们代码中包含的全局退火机制 (1 - global_progress)。这个机制虽然简单，但被证明有效。

显然，v10的策略是更加优雅的。幸运的是，将其与退火机制结合是可能的。v10的学习是基于同方差不确定性的，与lambda并无直接关系。也就是说，lambda仅仅是v10学习的结果，并不会反作用与v10的学习。那么，如果将这两个相对并不冲突的机制结合，我们可以预期获得一个在优雅的同时具有优秀结果的良好方案。  
