# 神奇的数字2.5，正负0.5

考虑到 lambda 退火策略的成效，我们自然会想到两个问题：

1. 不同的lambda值效果呈现什么变化趋势？
2. lambda 退火策略在不同数据集上的泛化能力如何？

为此我们进行以下实验：

我们在CIFAR10和SVHN上，取 alpha = 0.05 ，并分别取 lambda = 1.0 ,2.5 ,5.0 ,10.0 ,20.0 ,50.0 进行测试。



### 最终性能汇总表


| 算法 / Lambda | 最终准确率 (Acc) | **相对于基线 (V4) 的提升** | 
| :--- | :--- | :--- | 
| **OneshotOurs (V4, Baseline)** | **57.74%** | - |
| OursV7 (λ = 1.0) | 58.89% | +1.15% | 
| OursV7 (λ = 2.5) | 57.44% | -0.30% | 
| OursV7 (λ = 5.0) | 59.38% | +1.64% |
| OursV7 (λ = 10.0) | 59.39% | +1.65% |
| **OursV7 (λ = 20.0)** | **59.68%** | **+1.94%** | 
| OursV7 (λ = 50.0) | 59.39% | +1.65% |

| 算法 / Lambda | 最终准确率 (Acc) | **相对于基线 (V4) 的提升** |
| :--- | :--- | :--- |
| **OneshotOurs (V4, Baseline)** | **49.94%** | - |
| OursV7 (λ = 1.0) | **51.04%** | **+1.10%** |
| OursV7 (λ = 10.0) | 50.64% | +0.70% |
| **OursV7 (λ = 20.0)** | **51.07%** | **+1.13%** |
---


基于上述数据，我们可以构建对超参数 lambda 的敏感性分析。

无论采取什么数据集，最终准确率都相对于基线有明显的提升，并且随 lambda 的变化趋势相同，这充分说明了我们方法的通用性。  

然而， lambda 自身的变化对于准确率的影响十分有趣。结果基本呈现U形，即较低和较高的 lambda 的对齐效果良好，而中等的 lambda 效果不佳，甚至可能有反作用。除此之外，对于不同数据集，最适合的 lambda 并不相同。另外，似乎 lambda 对于较大数值的鲁棒性较好，对于 CIFAR10 ，即使到了 lambda = 50.0 ，性能依然维持在高位，没有出现显著下降，展现了宽顶平台型特征。


### 对比不同实验的学习曲线（Acc随round变化）


| idx |  N/A(V4)   |   1    |  2.5   |   5    |   10   |   20   |   50   |
|:---:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
|  0  | 0.2551 | 0.2389 | 0.2529 | 0.2394 | 0.2431 | 0.2444 | 0.2327 |
|  5  | 0.3315 | 0.3458 | 0.3617 | 0.3488 | 0.3561 | 0.3454 | 0.3543 |
| 10  | 0.3904 | 0.3764 | 0.3822 | 0.3734 | 0.3860 | 0.3823 | 0.3953 |
| 15  | 0.4062 | 0.4150 | 0.4213 | 0.4169 | 0.4255 | 0.4321 | 0.4398 |
| 20  | 0.4852 | 0.4650 | 0.4568 | 0.4655 | 0.4757 | 0.4864 | 0.4819 |
| 25  | 0.4947 | 0.5037 | 0.5073 | 0.5045 | 0.5108 | 0.5068 | 0.5182 |
| 30  | 0.5777 | 0.5745 | 0.5670 | 0.5719 | 0.5835 | 0.5771 | 0.5828 |
| 35  | 0.5649 | 0.5500 | 0.5413 | 0.5386 | 0.5571 | 0.5477 | 0.5609 |
| 40  | 0.5886 | 0.5568 | 0.5682 | 0.5483 | 0.5721 | 0.5723 | 0.5716 |
| 45  | 0.5717 | 0.5782 | 0.5687 | 0.5724 | 0.5811 | 0.5764 | 0.5789 |
| 49  | 0.5774 | 0.5889	| 0.5744 | 0.5877 | 0.5938 | 0.5968 | 0.5939 |


*   **基线 (V4):** 学习曲线波动较大，收敛速度中等。
*   **V7 (λ=1.0, 2.5):** 曲线形态与基线类似，稍显平滑，最终略高。
*   **V7 (λ=5.0, 10.0, 20.0, 50.0):**
    *   在前10轮，强 lambda 的模型准确率普遍高于弱 lambda 和基线。这表明强力的初始引导有助于模型快速找到正确的特征空间。
    *   整个学习过程非常平滑，几乎没有波动，展现了极高的训练稳定性。

下面我们对观察到的现象进行猜想。

## 结果基本呈现U形

我们猜测，对齐框架在不同 lambda 强度下，存在两种不同的工作机制：

已经有研究证明，即使是随机的锚点，也会对本地对齐有促进作用，并可缓解模型不一致性。

- 当lambda较低时，loss以本地对齐梯度为主， lambda=1.0 的全局对齐梯度，扮演了一个轻量级正则化项的角色。它足够强大，对本地对齐有促进作用，并可稍微纠正一下方向，但绝不足以挑战本地对齐梯度的主导地位。
- 当 lambda 较大时，align_loss 将原型拉向全局统一的、几何最优的ETF锚点。依照lambda退火机制，模型首先成为了一个结构良好的全局通用性模型，然后再利用本地数据进行精修。由于ETF结构本身可以从数学上证明最优，这个新范式下的模型性能大幅提升，并且表现出极强的鲁棒性。
- 当 lambda 大小不大不小时，破坏性干扰出现了。全局对齐梯度的力量现在，以至于本地对齐无法有效地完成其拟合本地数据的任务，导致 base_loss 降不下去。但全局对齐梯度的力量又不够强，无法原型完全地拉向全局ETF目标，导致 align_loss 也降不下去。这导致模型被困在了两种优化目标的冲突之中，性能反而下降。

## 对于不同任务需要不同lambda

*   **在CIFAR-10 (物体识别) 上：**
    *   本地任务是**语义复杂**的。本地和全局力量平衡非常**微妙**。过弱的 lambda 无法建立全局范式，过强的 lambda 可能破坏了对细腻语义特征的学习，因此出现了性能相对于baseline下降的区域。

*   **在SVHN (数字识别) 上：**
    *   本地任务是语义简单但感知困难的。区分“3”和“8”主要依赖于对形状、笔画等低级特征的学习。
    *   这意味着本地对齐的目标非常明确和“刚性”。只要 lambda 足够大（ ≥1.0 ），能够将系统推入“全局主导”的范式，模型就能从ETF锚点这个优秀的几何结构中获益。由于任务本身不那么依赖复杂的语义，即使是 lambda=20.0 的强约束，也**不会破坏**模型对“数字形状”这一核心信息的学习。因此，其对于lambda更加鲁棒。

## 结论

通过对对齐强度λ的系统性研究，我们猜测揭示了在本地学习与全局对齐的相互作用中，存在着两种截然不同的学习范式。  
在弱约束区 (λ≈1.0)，全局对齐扮演轻量级正则化的角色，对以本地学习为主导的范式进行温和改良。  
在强约束区 (λ≥5.0)，训练动力学发生根本性转移，转变为一个以全局结构为主导的全新范式，模型性能在该范式下达到最优和最稳定的状态。  
这为理解和设计联邦学习中的正则化与约束机制，提供了一个全新的的理论视角。