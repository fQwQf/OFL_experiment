# 2025/9/24 public feature bank 2

既然直接的`loss`相加是行不通的。那么可以解耦（Decouple） 这两个相互冲突的目标。参考 pre-train + fine-tune ，我们不再让模型同时学习两个冲突的目标，而是分步进行。

在最初的几个 `local_epochs` ，客户端只使用对比损失进行训练。在这个阶段，模型不关心本地分类任务，它的唯一目标是学习一个通用的特征提取器。在已经对齐的特征空间基础上，再学习本地的分类任务。由于模型已经有了一个很好的特征基础，这个阶段的微调会变得非常高效。模型可以在不破坏已对齐的全局结构的前提下，快速学习区分本地类别的细微差别。

原始实验数据见[public_feature_bank2.txt](public_feature_bank2.txt)。

实验结果明确显示，之前提出的两阶段训练方案（`OursV6`）在所有Non-IID设置下，性能均显著低于原始的FAFI基线（`OursV4`）。更重要的是，数据的Non-IID程度越强（`alpha`值越小），`OursV6`的性能下降越剧烈。

### 实验结果对整理

| Non-IID Level (alpha) | Round | OursV4 (Baseline) | OursV6 (Two-Stage) | Performance Delta (V6 - V4) |
| :--- | :---: | :---: | :---: | :---: |
| 0.5 (弱) | 0 | 67.23% | 57.68% | -9.55% |
| | 1 | 76.70% | 71.27% | -5.43% |
| | 2 | 81.13% | 76.92% | -4.21% |
| | 5 | 86.79% | 84.45% | -2.34% |
| 0.3 (中) | 0 | 65.16% | 47.65% | -17.51% |
| | 1 | 74.79% | 60.79% | -14.00% |
| | 2 | 79.09% | 66.91% | -12.18% |
| | 5 | 85.62% | 75.96% | -9.66% |
| 0.1 (强) | 0 | 61.00% | 45.38% | -15.62% |
| | 1 | 71.24% | 53.19% | -18.05% |
| | 2 | 76.98% | 59.88% | -17.10% |
| | 5 | 83.52% | 70.01% | -13.51% |
| 0.05 (极强) | 0 | 61.37% | 31.25% | -30.12% |
| | 1 | 72.25% | 38.67% | -33.58% |
| | 2 | 77.12% | 42.28% | -34.84% |
| | 5 | 83.46% | 50.13% | -33.33% |

### 结果分析

我们的核心假设是：通过设置一个专门的“对齐阶段”（Stage 1），可以让客户端模型先学习一个全局一致的特征表示，然后再在“微调阶段”（Stage 2）学习本地任务，从而避免梯度冲突。

实验结果否定这个假设。原因可能如下：

#### 1. 对齐阶段太短

在我们的设置中 (`local_epochs: 5`, `alignment_epochs: 1`)，客户端模型在随机初始化后，只用一个 epoch 的时间来进行全局对齐。

*   日志分析（`alpha=0.05`, Client 0, Epoch 0）:
    *   `Epoch 0: Running in Stage 1 - Global Alignment Only.`
    *   `train accuracy: 0.0; test accuracy: 0.1022`

    经过一个轮次的纯对齐训练后，模型在训练集和测试集上的表现等同于随机猜测。这意味着，这个所谓的“对齐”阶段，完全没有让模型学到任何有意义的特征。

#### 2. 本地任务迅速覆盖全局信号

当训练进入第二阶段（本地微调）时，一个几乎是随机的模型，突然被暴露在梯度极其巨大且方向单一的本地分类损失（`cls_loss`）之下。在第一阶段学到的全局对齐知识，会在第二阶段的第一个batch中被彻底冲刷和覆盖，强大的本地任务信号覆盖了微弱的全局预训练信号。


现在我们可以解释为什么`OursV6`甚至比基线更糟糕：

*   `OursV4`: 从第一个epoch开始，就同时接收`cls_loss`等所有损失的监督。尽管存在梯度冲突，但至少模型从一开始就在学习如何为本地数据进行分类，这是一个有意义的任务。它有5个完整的epoch来学习这个任务。
*   `OursV6`: 将宝贵的第一个epoch完全浪费在了一个无效的对齐任务上，这个任务结束后模型基本还是随机的。然后，它只剩下4个epoch来从零开始学习本地任务。

这个效应在`alpha=0.05`时最为惨烈，因为此时本地任务的过拟合倾向最强，模型最需要从一开始就建立正确的分类概念。而在`alpha=0.5`时，本地任务与全局任务较为接近，浪费一个epoch的负面影响相对较小，因此性能差距也最小。