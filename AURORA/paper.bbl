\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amato et~al.(2025)Amato, Qiu, Tanveer, Cuomo, Annunziata, Giampaolo,
  and Piccialli]{amato2025survey}
Amato, F., Qiu, L., Tanveer, M., Cuomo, S., Annunziata, D., Giampaolo, F., and
  Piccialli, F.
\newblock Towards one-shot federated learning: Advances, challenges, and future
  directions.
\newblock \emph{Neurocomputing}, pp.\  132088, 2025.

\bibitem[Chen et~al.(2018)Chen, Badrinarayanan, Lee, and
  Rabinovich]{chen2018gradnorm}
Chen, Z., Badrinarayanan, V., Lee, C.-Y., and Rabinovich, A.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In \emph{International conference on machine learning}, pp.\
  794--803. PMLR, 2018.

\bibitem[Dai et~al.()Dai, Zhang, Li, Liu, Yang, and Han]{dai2024coboosting}
Dai, R., Zhang, Y., Li, A., Liu, T., Yang, X., and Han, B.
\newblock Enhancing one-shot federated learning through data and ensemble
  co-boosting.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1165--1173. PMLR, 2017.

\bibitem[Guha et~al.(2019)Guha, Talwalkar, and Smith]{guha2019}
Guha, N., Talwalkar, A., and Smith, V.
\newblock One-shot federated learning.
\newblock \emph{arXiv preprint arXiv:1902.11175}, 2019.

\bibitem[He et~al.(2025)He, Tong, Fang, Sun, Zeng, Li, Chen, and
  Zhuang]{AFL_He_CVPR2025}
He, R., Tong, K., Fang, D., Sun, H., Zeng, Z., Li, H., Chen, T., and Zhuang, H.
\newblock Afl: A single-round analytic approach for federated learning with
  pre-trained models.
\newblock In \emph{Proceedings of the Computer Vision and Pattern Recognition
  Conference}, pp.\  4988--4998, 2025.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International conference on machine learning}, pp.\
  5132--5143. PMLR, 2020.

\bibitem[Kendall et~al.(2018)Kendall, Gal, and Cipolla]{kendall2018}
Kendall, A., Gal, Y., and Cipolla, R.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  7482--7491, 2018.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine learning and systems}, 2:\penalty0
  429--450, 2020.

\bibitem[Li et~al.(2023)Li, Shang, He, Lin, and Wu]{li2023}
Li, Z., Shang, X., He, R., Lin, T., and Wu, C.
\newblock No fear of classifier biases: Neural collapse inspired federated
  learning with synthetic and fixed classifier.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  5319--5329, 2023.

\bibitem[Liu et~al.(2019)Liu, Johns, and Davison]{liu2019mtan}
Liu, S., Johns, E., and Davison, A.~J.
\newblock End-to-end multi-task learning with attention.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  1871--1880, 2019.

\bibitem[Liu et~al.(2026)Liu, Zhang, Wang, Zhu, and Luo]{liu2026feature}
Liu, S., Zhang, H., Wang, X., Zhu, Y., and Luo, G.
\newblock Feature-aware one-shot federated learning via hierarchical token
  sequences.
\newblock \emph{arXiv preprint arXiv:2601.03882}, 2026.

\bibitem[Liu et~al.(2024)Liu, Liu, Ye, Shen, Li, Jiang, and Li]{liu2024}
Liu, X., Liu, L., Ye, F., Shen, Y., Li, X., Jiang, L., and Li, J.
\newblock Fedlpa: One-shot federated learning with layer-wise posterior
  aggregation.
\newblock \emph{Advances in Neural Information Processing Systems},
  37:\penalty0 81510--81548, 2024.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020}
Papyan, V., Han, X., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Tan et~al.(2022)Tan, Long, Liu, Zhou, Lu, Jiang, and Zhang]{tan2022}
Tan, Y., Long, G., Liu, L., Zhou, T., Lu, Q., Jiang, J., and Zhang, C.
\newblock Fedproto: Federated prototype learning across heterogeneous clients.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~36, pp.\  8432--8440, 2022.

\bibitem[Tang et~al.(2024)Tang, Zhang, Dong, Cheung, Zhou, Han, and
  Chu]{tang2024fusefl}
Tang, Z., Zhang, Y., Dong, P., Cheung, Y.-m., Zhou, A., Han, B., and Chu, X.
\newblock Fusefl: One-shot federated learning through the lens of causality
  with progressive model fusion.
\newblock \emph{Advances in Neural Information Processing Systems},
  37:\penalty0 28393--28429, 2024.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020pcgrad}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 5824--5836, 2020.

\bibitem[Zeng et~al.()Zeng, Huang, Zhou, Wu, Wan, Chen, and Cai]{zeng2025}
Zeng, H., Huang, W., Zhou, T., Wu, X., Wan, G., Chen, Y., and Cai, Z.
\newblock Does one-shot give the best shot? mitigating model inconsistency in
  one-shot federated learning.
\newblock In \emph{Forty-second International Conference on Machine Learning}.

\bibitem[Zeng et~al.(2024)Zeng, Xu, Zhou, Wu, Kang, Cai, and
  Niyato]{zeng2024one}
Zeng, H., Xu, M., Zhou, T., Wu, X., Kang, J., Cai, Z., and Niyato, D.
\newblock One-shot-but-not-degraded federated learning.
\newblock In \emph{Proceedings of the 32nd ACM International Conference on
  Multimedia}, pp.\  11070--11079, 2024.

\bibitem[Zeng et~al.(2025)Zeng, Lou, Wang, Zhou, Wu, Zhao, and
  Li]{zeng2025bapfl}
Zeng, H., Lou, J., Wang, Z., Zhou, H., Wu, C., Zhao, W., and Li, J.
\newblock Bapfl: Exploring backdoor attacks against prototype-based federated
  learning.
\newblock \emph{arXiv preprint arXiv:2509.12964}, 2025.

\bibitem[Zhang et~al.(2022)Zhang, Chen, Li, Lyu, Wu, Ding, Shen, and
  Wu]{zhang2022}
Zhang, J., Chen, C., Li, B., Lyu, L., Wu, S., Ding, S., Shen, C., and Wu, C.
\newblock Dense: Data-free one-shot federated learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 21414--21428, 2022.

\bibitem[Zhang et~al.(2024)Zhang, Liu, Hua, and Cao]{zhang2024}
Zhang, J., Liu, Y., Hua, Y., and Cao, J.
\newblock Fedtgp: Trainable global prototypes with adaptive-margin-enhanced
  contrastive learning for data and model heterogeneity in federated learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~38, pp.\  16768--16776, 2024.

\end{thebibliography}
