\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amato et~al.(2025)Amato, Qiu, Tanveer, Cuomo, Giampaolo, and
  Piccialli]{amato2025survey}
Amato, F., Qiu, L., Tanveer, M., Cuomo, S., Giampaolo, F., and Piccialli, F.
\newblock Towards one-shot federated learning: Advances, challenges, and future
  directions.
\newblock \emph{arXiv preprint arXiv:2505.02426}, 2025.

\bibitem[Chen et~al.(2018)Chen, Badrinarayanan, Lee, and
  Rabinovich]{chen2018gradnorm}
Chen, Z., Badrinarayanan, V., Lee, C.-Y., and Rabinovich, A.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  794--803. PMLR, 2018.

\bibitem[Cipolla et~al.(2018)Cipolla, Gal, and Kendall]{kendall2018}
Cipolla, R., Gal, Y., and Kendall, A.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  7482--7491, 2018.
\newblock \doi{10.1109/CVPR.2018.00781}.

\bibitem[Dai et~al.(2024)]{dai2024coboosting}
Dai, R. et~al.
\newblock Enhancing one-shot federated learning through data and ensemble
  co-boosting.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2024.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Guha et~al.(2019)Guha, Talwalkar, and Smith]{guha2019}
Guha, N., Talwalkar, A., and Smith, V.
\newblock One-shot federated learning.
\newblock \emph{arXiv preprint arXiv:1902.11175}, 2019.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5132--5143. PMLR, 2020.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  429--450, 2020.

\bibitem[Li et~al.(2023)Li, Shang, He, Lin, and Wu]{li2023}
Li, Z., Shang, X., He, R., Lin, T., and Wu, C.
\newblock No fear of classifier biases: Neural collapse inspired federated
  learning with synthetic and fixed classifier.
\newblock In \emph{2023 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pp.\  5296--5306, 2023.
\newblock \doi{10.1109/ICCV51070.2023.00490}.

\bibitem[Liu et~al.(2019)Liu, Johns, and Davison]{liu2019mtan}
Liu, S., Johns, E., and Davison, A.~J.
\newblock End-to-end multi-task learning with attention.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  1871--1880, 2019.

\bibitem[Liu et~al.(2026)Liu, Zhang, Wang, Zhu, and Luo]{liu2026feature}
Liu, S., Zhang, H., Wang, X., Zhu, Y., and Luo, G.
\newblock Feature-aware one-shot federated learning via hierarchical token
  sequences.
\newblock \emph{arXiv preprint arXiv:2601.03882}, 2026.

\bibitem[Liu et~al.(2024)Liu, Liu, Ye, Shen, Li, Jiang, and Li]{liu2024}
Liu, X., Liu, L., Ye, F., Shen, Y., Li, X., Jiang, L., and Li, J.
\newblock Fedlpa: One-shot federated learning with layer-wise posterior
  aggregation.
\newblock \emph{Advances in Neural Information Processing Systems},
  37:\penalty0 81510--81548, 2024.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020}
Papyan, V., Han, X.~Y., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.
\newblock \doi{10.1073/pnas.2015509117}.

\bibitem[Tan et~al.(2022)Tan, Long, Liu, Zhou, Lu, Jiang, and Zhang]{tan2022}
Tan, Y., Long, G., Liu, L., Zhou, T., Lu, Q., Jiang, J., and Zhang, C.
\newblock Fedproto: Federated prototype learning across heterogeneous clients.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  8432--8440, 2022.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020pcgrad}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zeng et~al.(2025{\natexlab{a}})Zeng, Huang, Zhou, Wu, Wan, Chen, and
  Cai]{zeng2025}
Zeng, H., Huang, W., Zhou, T., Wu, X., Wan, G., Chen, Y., and Cai, Z.
\newblock Does one-shot give the best shot? mitigating model inconsistency in
  one-shot federated learning.
\newblock In \emph{International Conference on Machine Learning},
  2025{\natexlab{a}}.

\bibitem[Zeng et~al.(2025{\natexlab{b}})Zeng, Lou, Wang, Zhou, Wu, Zhao, and
  Li]{zeng2025bapfl}
Zeng, H., Lou, J., Wang, Z., Zhou, H., Wu, C., Zhao, W., and Li, J.
\newblock Bapfl: Exploring backdoor attacks against prototype-based federated
  learning.
\newblock \emph{arXiv preprint arXiv:2509.12964}, 2025{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Chen, Li, Lyu, Wu, Ding, Shen, and
  Wu]{zhang2022}
Zhang, J., Chen, C., Li, B., Lyu, L., Wu, S., Ding, S., Shen, C., and Wu, C.
\newblock Dense: Data-free one-shot federated learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 21414--21428, 2022.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Liu, and Wang]{zhang2024one}
Zhang, J., Liu, S., and Wang, X.
\newblock One-shot federated learning via synthetic distiller-distillate
  communication.
\newblock \emph{Advances in Neural Information Processing Systems},
  37:\penalty0 102611--102633, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Liu, Hua, and Cao]{zhang2024}
Zhang, J., Liu, Y., Hua, Y., and Cao, J.
\newblock Fedtgp: Trainable global prototypes with adaptive-margin-enhanced
  contrastive learning for data and model heterogeneity in federated learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~38, pp.\  16768--16776, 2024{\natexlab{b}}.

\end{thebibliography}
