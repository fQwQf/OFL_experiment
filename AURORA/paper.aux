\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mcmahan2017}
\citation{guha2019}
\citation{zeng2025}
\citation{zeng2025}
\citation{papyan2020}
\citation{amato2025survey}
\citation{li2020}
\citation{karimireddy2020}
\citation{zhang2022}
\citation{dai2024coboosting}
\citation{zhang2024one}
\citation{liu2024}
\citation{zeng2025}
\citation{liu2026feature}
\citation{zeng2025bapfl}
\citation{tan2022}
\citation{zhang2024}
\citation{papyan2020}
\citation{li2023}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:motivation}{{1}{2}{The Motivation: Visualizing the "Temporal Dichotomy" in One-Shot FL. (Left) The Failure of Static Alignment: While strictly anchoring local prototypes to a global ETF structure (large $\lambda $) prevents feature drift in the early stage, it becomes too rigid in the late stage, prohibiting necessary local adaptation and causing model collapse. (Right) The Dynamic Need: We identify that the optimal alignment strength is time-varying. A robust system requires strong global guidance initially to fix the coordinate system, followed by autonomous relaxation (green waves) to enable fine-grained feature learning. AURORA automates this trajectory without manual tuning}{figure.caption.1}{}}
\newlabel{fig:motivation@cref}{{[figure][1][]1}{[1][1][]2}{}{}{}}
\citation{li2023}
\citation{zeng2025}
\citation{kendall2018}
\citation{yu2020pcgrad}
\citation{franceschi2017}
\citation{papyan2020}
\citation{kendall2018}
\newlabel{fig:method}{{2}{4}{\textbf {The AURORA Framework.} \textbf {(Left) Client-Side Autonomous Alignment (Training Phase):} The architecture is decoupled into three layers: the \textit {Foundation Layer} handles local task learning; the \textit {Alignment Layer} introduces global Simplex ETF anchors; and the \textit {Control Layer} serves as an autonomous regulator using meta-annealing and uncertainty-based weighting to dynamically adjust $\lambda _{eff}$ through a gradient decoupling barrier. \textbf {(Right) Server-Side One-shot Aggregation (Testing Phase):} After local training, clients upload aligned prototypes to the server for a unified global model fusion without iterative communication}{figure.caption.2}{}}
\newlabel{fig:method@cref}{{[figure][2][]2}{[1][3][]4}{}{}{}}
\citation{zeng2025}
\citation{liu2024}
\citation{zeng2025}
\newlabel{tab:main-results-c100}{{1}{7}{Test Accuracy (\%) on CIFAR-100 ($\alpha =0.05$). AURORA significantly outperforms baselines and matches manually-tuned annealing with lower variance}{table.caption.3}{}}
\newlabel{tab:main-results-c100@cref}{{[table][1][]1}{[1][6][]7}{}{}{}}
\newlabel{tab:main-results-others}{{2}{7}{Test Accuracy (\%) on Other Settings. AURORA consistently outperforms baselines}{table.caption.4}{}}
\newlabel{tab:main-results-others@cref}{{[table][2][]2}{[1][6][]7}{}{}{}}
\newlabel{fig:acc-vs-alpha}{{3}{7}{Accuracy vs Heterogeneity (CIFAR-10). AURORA consistently outperforms baselines across all heterogeneity levels. Higher $\alpha $ means less heterogeneity (easier)}{figure.caption.5}{}}
\newlabel{fig:acc-vs-alpha@cref}{{[figure][3][]3}{[1][7][]7}{}{}{}}
\newlabel{tab:consistency}{{3}{7}{Model Consistency (g\_protos\_std $\downarrow $) on CIFAR-10 ($\alpha $=0.05)}{table.caption.6}{}}
\newlabel{tab:consistency@cref}{{[table][3][]3}{[1][7][]7}{}{}{}}
\newlabel{tab:ablation}{{4}{7}{Ablation Study on CIFAR-100 ($\alpha $=0.05)}{table.caption.7}{}}
\newlabel{tab:ablation@cref}{{[table][4][]4}{[1][7][]7}{}{}{}}
\newlabel{tab:fedavg-results}{{5}{8}{Aggregator Robustness: Test Accuracy (\%) with \textbf {FedAvg} (CIFAR-10, $\alpha $=0.1). AURORA improves pure parameter averaging}{table.caption.8}{}}
\newlabel{tab:fedavg-results@cref}{{[table][5][]5}{[1][8][]8}{}{}{}}
\newlabel{tab:lambda-evolution}{{6}{8}{$\lambda $ Evolution Comparison (CIFAR-100, $\alpha $=0.05). Full trajectory analysis in Appendix~\ref {appendix:schedule-analysis}}{table.caption.9}{}}
\newlabel{tab:lambda-evolution@cref}{{[table][6][]6}{[1][8][]8}{}{}{}}
\newlabel{sec:discussion}{{5}{8}{}{section.5}{}}
\newlabel{sec:discussion@cref}{{[section][5][]5}{[1][8][]8}{}{}{}}
\bibdata{references}
\bibcite{amato2025survey}{{1}{2025}{{Amato et~al.}}{{Amato, Qiu, Tanveer, Cuomo, Giampaolo, and Piccialli}}}
\bibcite{chen2018gradnorm}{{2}{2018}{{Chen et~al.}}{{Chen, Badrinarayanan, Lee, and Rabinovich}}}
\bibcite{kendall2018}{{3}{2018}{{Cipolla et~al.}}{{Cipolla, Gal, and Kendall}}}
\bibcite{dai2024coboosting}{{4}{2024}{{Dai et~al.}}{{}}}
\bibcite{franceschi2017}{{5}{2017}{{Franceschi et~al.}}{{Franceschi, Donini, Frasconi, and Pontil}}}
\bibcite{guha2019}{{6}{2019}{{Guha et~al.}}{{Guha, Talwalkar, and Smith}}}
\bibcite{karimireddy2020}{{7}{2020}{{Karimireddy et~al.}}{{Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh}}}
\bibcite{li2020}{{8}{2020}{{Li et~al.}}{{Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith}}}
\bibcite{li2023}{{9}{2023}{{Li et~al.}}{{Li, Shang, He, Lin, and Wu}}}
\bibcite{liu2019mtan}{{10}{2019}{{Liu et~al.}}{{Liu, Johns, and Davison}}}
\bibcite{liu2026feature}{{11}{2026}{{Liu et~al.}}{{Liu, Zhang, Wang, Zhu, and Luo}}}
\bibcite{liu2024}{{12}{2024}{{Liu et~al.}}{{Liu, Liu, Ye, Shen, Li, Jiang, and Li}}}
\bibcite{mcmahan2017}{{13}{2017}{{McMahan et~al.}}{{McMahan, Moore, Ramage, Hampson, and y~Arcas}}}
\bibcite{papyan2020}{{14}{2020}{{Papyan et~al.}}{{Papyan, Han, and Donoho}}}
\bibcite{tan2022}{{15}{2022}{{Tan et~al.}}{{Tan, Long, Liu, Zhou, Lu, Jiang, and Zhang}}}
\bibcite{yu2020pcgrad}{{16}{2020}{{Yu et~al.}}{{Yu, Kumar, Gupta, Levine, Hausman, and Finn}}}
\bibcite{zeng2025}{{17}{2025{a}}{{Zeng et~al.}}{{Zeng, Huang, Zhou, Wu, Wan, Chen, and Cai}}}
\bibcite{zeng2025bapfl}{{18}{2025{b}}{{Zeng et~al.}}{{Zeng, Lou, Wang, Zhou, Wu, Zhao, and Li}}}
\bibcite{zhang2022}{{19}{2022}{{Zhang et~al.}}{{Zhang, Chen, Li, Lyu, Wu, Ding, Shen, and Wu}}}
\bibcite{zhang2024one}{{20}{2024{a}}{{Zhang et~al.}}{{Zhang, Liu, and Wang}}}
\bibcite{zhang2024}{{21}{2024{b}}{{Zhang et~al.}}{{Zhang, Liu, Hua, and Cao}}}
\bibstyle{icml2026}
\citation{kendall2018}
\citation{chen2018gradnorm}
\newlabel{tab:gradnorm-comparison}{{7}{13}{Comparison between GradNorm and AURORA}{table.caption.12}{}}
\newlabel{tab:gradnorm-comparison@cref}{{[table][7][]7}{[1][13][]13}{}{}{}}
\citation{liu2024}
\newlabel{tab:sigma-lr}{{8}{14}{Effect of $\sigma $ learning rate on CIFAR-100 ($\alpha $=0.05)}{table.caption.13}{}}
\newlabel{tab:sigma-lr@cref}{{[table][8][]8}{[1][14][]14}{}{}{}}
\newlabel{tab:lambda-max}{{9}{14}{Effect of $\lambda _{\max }$ threshold on CIFAR-100 ($\alpha $=0.05)}{table.caption.14}{}}
\newlabel{tab:lambda-max@cref}{{[table][9][]9}{[1][14][]14}{}{}{}}
\citation{liu2024}
\citation{liu2024}
\citation{liu2024}
\citation{kendall2018}
\citation{chen2018gradnorm}
\citation{liu2019mtan}
\citation{yu2020pcgrad}
\newlabel{tab:lambda-no-constraint}{{10}{15}{Per-client Raw $\lambda $ trajectory \textbf {without} $\lambda $-ReLU constraint on SVHN}{table.caption.15}{}}
\newlabel{tab:lambda-no-constraint@cref}{{[table][10][]10}{[1][14][]15}{}{}{}}
\newlabel{tab:lambda-with-constraint}{{11}{15}{Per-client Raw $\lambda $ trajectory \textbf {with} $\lambda $-ReLU constraint ($\lambda _{\max }$=50)}{table.caption.16}{}}
\newlabel{tab:lambda-with-constraint@cref}{{[table][11][]11}{[1][14][]15}{}{}{}}
\newlabel{appendix:extended-related}{{F}{15}{Impact Statement}{appendix.F}{}}
\newlabel{appendix:extended-related@cref}{{[section][6][]F}{[1][14][]15}{}{}{}}
\newlabel{fig:lambda-explosion}{{4}{16}{$\lambda $ Explosion on SVHN ($\alpha $=0.05). Under extreme heterogeneity, the unregularized uncertainty objective drives $\lambda $ toward infinity ($>1.6 \times 10^6$). AURORA's stability regularization effectively anchors $\lambda $ within a functional range}{figure.caption.17}{}}
\newlabel{fig:lambda-explosion@cref}{{[figure][4][]4}{[1][14][]16}{}{}{}}
\newlabel{tab:data-skew-correlation}{{12}{16}{Correlation between data entropy and $\lambda $ trajectory at Round 19}{table.caption.18}{}}
\newlabel{tab:data-skew-correlation@cref}{{[table][12][]12}{[1][14][]16}{}{}{}}
\newlabel{appendix:formal-analysis}{{G}{16}{Impact Statement}{appendix.G}{}}
\newlabel{appendix:formal-analysis@cref}{{[section][7][]G}{[1][15][]16}{}{}{}}
\newlabel{tab:fedlpa-results}{{13}{17}{FedLPA reported performance. \textbf {Note: Values are reported directly from the original paper~\cite {liu2024} and were not reproduced in this work.} FedLPA uses Dirichlet parameter $\beta $ (equivalent to our $\alpha $)}{table.caption.19}{}}
\newlabel{tab:fedlpa-results@cref}{{[table][13][]13}{[1][15][]17}{}{}{}}
\newlabel{appendix:impl-details}{{H}{17}{Impact Statement}{appendix.H}{}}
\newlabel{appendix:impl-details@cref}{{[section][8][]H}{[1][17][]17}{}{}{}}
\citation{zeng2025}
\citation{liu2024}
\newlabel{appendix:schedule-analysis}{{I}{19}{Impact Statement}{appendix.I}{}}
\newlabel{appendix:schedule-analysis@cref}{{[section][9][]I}{[1][18][]19}{}{}{}}
\newlabel{tab:lambda-evolution-appendix}{{14}{19}{$\lambda $ Evolution Comparison (CIFAR-100, $\alpha $=0.05)}{table.caption.20}{}}
\newlabel{tab:lambda-evolution-appendix@cref}{{[table][14][]14}{[1][18][]19}{}{}{}}
\newlabel{fig:lambda-divergence-appendix}{{5}{19}{Per-Client $\lambda $ Divergence. Despite sharing the same $s(p)$ prior, clients develop divergent $\lambda $ trajectories based on their local data characteristics. By checkpoint 8, Client 4's $\lambda $ is 51\% higher than Client 0's---demonstrating AURORA is \emph {data-dependent}, not merely \emph {time-dependent}}{figure.caption.21}{}}
\newlabel{fig:lambda-divergence-appendix@cref}{{[figure][5][]5}{[1][19][]19}{}{}{}}
\newlabel{appendix:robustness}{{J}{19}{Impact Statement}{appendix.J}{}}
\newlabel{appendix:robustness@cref}{{[section][10][]J}{[1][19][]19}{}{}{}}
\newlabel{tab:svhn-robustness-appendix}{{15}{19}{SVHN Performance Under Extreme Heterogeneity ($\alpha $=0.05)}{table.caption.22}{}}
\newlabel{tab:svhn-robustness-appendix@cref}{{[table][15][]15}{[1][19][]19}{}{}{}}
\newlabel{appendix:hyperparam}{{K}{19}{Impact Statement}{appendix.K}{}}
\newlabel{appendix:hyperparam@cref}{{[section][11][]K}{[1][19][]19}{}{}{}}
\newlabel{tab:hyperparam-comparison-appendix}{{16}{20}{Comparison of hyperparameter types}{table.caption.23}{}}
\newlabel{tab:hyperparam-comparison-appendix@cref}{{[table][16][]16}{[1][19][]20}{}{}{}}
\newlabel{tab:sensitivity-appendix}{{17}{20}{Sensitivity Analysis on SVHN ($\alpha $=0.05)}{table.caption.24}{}}
\newlabel{tab:sensitivity-appendix@cref}{{[table][17][]17}{[1][20][]20}{}{}{}}
\newlabel{tab:lambda-sensitivity-appendix}{{18}{20}{Effect of Fixed $\lambda $ with Linear Annealing on CIFAR-10 ($\alpha $=0.05)}{table.caption.25}{}}
\newlabel{tab:lambda-sensitivity-appendix@cref}{{[table][18][]18}{[1][20][]20}{}{}{}}
\newlabel{appendix:scalability}{{L}{20}{Impact Statement}{appendix.L}{}}
\newlabel{appendix:scalability@cref}{{[section][12][]L}{[1][20][]20}{}{}{}}
\newlabel{tab:scalability-appendix}{{19}{21}{Performance with Varying Number of Clients on CIFAR-10 ($\alpha $=0.1)}{table.caption.26}{}}
\newlabel{tab:scalability-appendix@cref}{{[table][19][]19}{[1][20][]21}{}{}{}}
\newlabel{appendix:projector-analysis}{{M}{21}{Impact Statement}{appendix.M}{}}
\newlabel{appendix:projector-analysis@cref}{{[section][13][]M}{[1][21][]21}{}{}{}}
\newlabel{tab:projector-impact}{{20}{21}{Impact of Projector under Dimensionality Bottleneck (CIFAR-100, $d=32$). Adding a projector significantly recovers performance by relieving the geometric bottleneck}{table.caption.27}{}}
\newlabel{tab:projector-impact@cref}{{[table][20][]20}{[1][21][]21}{}{}{}}
\gdef \@abspage@last{21}
